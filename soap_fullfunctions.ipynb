{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFFTiH4D/pNJbjsZ1n3XKU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sasurasa/soda_ep1/blob/main/soap_fullfunctions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn==1.4.0\n",
        "!pip install scikit-survival\n",
        "!pip install lifelines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WeNXBgc6eYOa",
        "outputId": "1043d22d-2d58-47fc-9152-78385c8b6311"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==1.4.0\n",
            "  Using cached scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.0) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.0) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.0) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.0) (3.3.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.3.2\n",
            "    Uninstalling scikit-learn-1.3.2:\n",
            "      Successfully uninstalled scikit-learn-1.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scikit-survival 0.22.2 requires scikit-learn<1.4,>=1.3.0, but you have scikit-learn 1.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scikit-learn-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              },
              "id": "ab03450c373b499d8ca2e6dd3c8649c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-survival in /usr/local/lib/python3.10/dist-packages (0.22.2)\n",
            "Requirement already satisfied: ecos in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (2.0.13)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (1.3.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (2.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (1.25.2)\n",
            "Requirement already satisfied: osqp!=0.6.0,!=0.6.1 in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (0.6.2.post8)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (1.5.3)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (1.11.4)\n",
            "Collecting scikit-learn<1.4,>=1.3.0 (from scikit-survival)\n",
            "  Using cached scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp!=0.6.0,!=0.6.1->scikit-survival) (0.1.7.post0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->scikit-survival) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->scikit-survival) (2023.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.4,>=1.3.0->scikit-survival) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.5->scikit-survival) (1.16.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.4.0\n",
            "    Uninstalling scikit-learn-1.4.0:\n",
            "      Successfully uninstalled scikit-learn-1.4.0\n",
            "Successfully installed scikit-learn-1.3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              },
              "id": "ac68e619a24143f3b223de3f825d884b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lifelines in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from lifelines) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from lifelines) (1.11.4)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from lifelines) (1.5.3)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from lifelines) (3.7.1)\n",
            "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.10/dist-packages (from lifelines) (1.6.2)\n",
            "Requirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.10/dist-packages (from lifelines) (0.5.0)\n",
            "Requirement already satisfied: formulaic>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from lifelines) (1.0.1)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd>=1.5->lifelines) (0.18.3)\n",
            "Requirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines) (4.10.0)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines) (1.14.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->lifelines) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BpTAKUPGcuxb"
      },
      "outputs": [],
      "source": [
        "#Import basic packages from Colab\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import shapiro\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from scipy import stats\n",
        "from scipy.stats import ttest_ind\n",
        "from scipy.stats import mannwhitneyu\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import chi2_contingency\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sksurv.nonparametric import kaplan_meier_estimator\n",
        "from lifelines import KaplanMeierFitter\n",
        "from lifelines.statistics import logrank_test\n",
        "\n",
        "def soap_sheetin(path, sheetname = 'Sheet1'):\n",
        "\tdata = pd.read_excel(path, sheet_name=sheetname, engine='openpyxl')\n",
        "\treturn data\n",
        "\n",
        "def soap_explore(data): #Display all column names, data types and dimension\n",
        "\tsize = data.size\n",
        "\tdimension = data.shape\n",
        "\tvariables = data.columns.values.tolist()\n",
        "\tprint('===============================================================================================================')\n",
        "\tprint('\\nThe dataframe has',size, 'cells, with',dimension,'(row x column) dimension.\\n')\n",
        "\tprint('All the variables include;',variables,'\\n')\n",
        "\tprint('===============================================================================================================')\n",
        "\tprint('Types and numbers of each variables are;')\n",
        "\tprint(data.info())\n",
        "\n",
        "def soap_describe(data):\n",
        "\tfor var in data.columns.values.tolist():\n",
        "\t\tif data[var].nunique() > 2 and data[var].dtypes != 'O':\n",
        "\t\t\t\tprint(var)\n",
        "\t\t\t\tdes = (data[var].describe())\n",
        "\t\t\t\tprint(des)\n",
        "\t\t\t\tplt.figure(figsize=(5, 2))\n",
        "\t\t\t\tsns.boxplot(x=data[var])\n",
        "\t\t\t\tplt.title(f'Boxplot of {var}')\n",
        "\t\t\t\tplt.xlabel(var)\n",
        "\t\t\t\tplt.show()\n",
        "\t\t\t\tprint('-------------------------------------------------------------------------------------------------------------')\n",
        "\n",
        "#Counting missing values in each column\n",
        "def soap_unique_null(data):\n",
        "    unique_values = {}\n",
        "    null_count = data.isnull().sum().to_list()\n",
        "    for col in data.columns:\n",
        "        unique_values[col] = data[col].value_counts().shape[0]\n",
        "    unique = pd.DataFrame(unique_values, index=['unique value count']).transpose()\n",
        "    unique['number of null'] = null_count\n",
        "    print(unique)\n",
        "\n",
        "#Counting percent of categorical data\n",
        "def soap_count_percent(data, col):\n",
        "  abs_count = data[col].value_counts()\n",
        "  rel_count = data[col].value_counts(normalize=True)*100\n",
        "  count_tab = pd.DataFrame({'abs_count' : abs_count, 'percent' : rel_count})\n",
        "  count_tab = count_tab.sort_index(ascending=True)\n",
        "  count_tab.index.name = col\n",
        "  print(count_tab)\n",
        "  plt.figure(figsize=(5, 3))\n",
        "  sns.barplot(x=col, y='percent', data=count_tab)\n",
        "  plt.ylabel('Percentage')\n",
        "  plt.title(f'Percentage of each value in {col}')\n",
        "  plt.show()\n",
        "\n",
        "#Counting percent of all categorical data\n",
        "def soap_batch_percent(data):\n",
        "  for i in data.columns:\n",
        "    if data[i].nunique() < 6:\n",
        "      print('------------------------------------------------------------------')\n",
        "      soap_count_percent(data, i)\n",
        "\n",
        "#Exploring if the data has normal distribution or not\n",
        "def shapif(data):\n",
        "    data_numeric = data.select_dtypes(exclude=['object'])\n",
        "\n",
        "    d1 = {}\n",
        "    for i in data_numeric.columns.values.tolist():\n",
        "        data_numeric[i].astype('float64')\n",
        "        x = data_numeric[i].dropna()\n",
        "        s = shapiro(x)\n",
        "        # Rounding each element of the tuple to 3 decimal places\n",
        "        s_rounded = (round(s[0], 3), round(s[1], 3))\n",
        "        d1[i] = s_rounded\n",
        "    df1 = pd.DataFrame(d1.items(), columns=['variable', 'Shapiro-Wilk result'])\n",
        "    print(df1)\n",
        "\n",
        "\n",
        "def soaplore(data):\n",
        "    num_col = []\n",
        "    for i in data.columns:\n",
        "      if data[i].nunique() > 5:\n",
        "        num_col.append(i)\n",
        "    data = data[num_col]\n",
        "    shapif(data)\n",
        "    print('\\n')\n",
        "\n",
        "#Create one-hot encoder columns from a categorial column\n",
        "def soap_onehot(data, col):\n",
        "  encoder = OneHotEncoder(sparse=False)\n",
        "  onehot_encoded_data = encoder.fit_transform(data[[col]])\n",
        "  onehot_encoded_df = pd.DataFrame(onehot_encoded_data, columns=encoder.get_feature_names_out([col])).astype(int)\n",
        "  result = pd.concat([data, onehot_encoded_df], axis=1)\n",
        "  return result\n",
        "\n",
        "#Change continuous variable to binary\n",
        "def soap_genbi(data, var, cutoff):\n",
        "  var_gr = []\n",
        "  for i in data[var]:\n",
        "    if i < cutoff:\n",
        "      var_gr.append(0)\n",
        "    if i >= cutoff:\n",
        "      var_gr.append(1)\n",
        "  data[f'{var}_{cutoff}'] = var_gr\n",
        "  data.groupby(f'{var}_{cutoff}').count()\n",
        "\n",
        "#Find correlation with a target binary\n",
        "def soap_target_corr(data, target):\n",
        "  data.drop(target, axis=1).corrwith(data[target]).plot(kind='bar', grid=True, figsize=(20, 8) , title=\"Correlation with\"+target, color=\"Purple\")\n",
        "\n",
        "#Heatmap for binary column correlations\n",
        "def soap_heatmap_corr(data):\n",
        "  bi_cols = []\n",
        "  for col in data.columns:\n",
        "    if data[col].nunique() == 2:\n",
        "      bi_cols.append(col)\n",
        "\n",
        "  corr = data[bi_cols].corr()\n",
        "\n",
        "  plt.figure(figsize=(20, 20))  # Adjust the figure size as needed\n",
        "  sns.heatmap(corr,\n",
        "              xticklabels=corr.columns,\n",
        "              yticklabels=corr.columns,\n",
        "              annot=True,      # Annotate each cell with the numeric value\n",
        "              cmap='coolwarm', # Color map\n",
        "              linewidths=.5)   # Line widths between cells\n",
        "\n",
        "  plt.title('Heatmap of Correlation Among Attributes')\n",
        "  plt.show()\n",
        "\n",
        "#Return a list of binary column names\n",
        "def soap_bicol_list(data):\n",
        "    bi_cols = []\n",
        "    for col in data.columns:\n",
        "      if data[col].nunique() == 2:\n",
        "        bi_cols.append(col)\n",
        "    return bi_cols\n",
        "\n",
        "def chi_pv(data, outcome, factor):\n",
        "  table = pd.crosstab(data[outcome], data[factor])\n",
        "  c, p, dof, expected = chi2_contingency(table)\n",
        "  return p\n",
        "\n",
        "def soap_x_across(data, outcome):\n",
        "    d = {}\n",
        "    for factor in data.columns.values.tolist():\n",
        "        if data[factor].nunique() > 5:\n",
        "            continue\n",
        "        elif factor == outcome:\n",
        "            continue\n",
        "        else:\n",
        "            pv=chi_pv(data, outcome, factor)\n",
        "            d[factor] = pv\n",
        "    df = pd.DataFrame(d.items(), columns=['variable', 'Chisquare p-value'])\n",
        "    print(df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def n_percent(data, col):\n",
        "  display = []\n",
        "  abs_count = data[col].value_counts().sort_index()\n",
        "  rel_count = data[col].value_counts(normalize=True).sort_index() * 100\n",
        "  for i in range(data[col].nunique()):\n",
        "    display_figure = f\"{abs_count.iloc[i]} ({rel_count.iloc[i]:.2f}%)\"\n",
        "    display.append(display_figure)\n",
        "  return display\n",
        "\n",
        "#Chi-square test\n",
        "\n",
        "def soap_x_tab(data, var_a, var_b):\n",
        "    table = pd.crosstab(data[var_b], data[var_a])\n",
        "    c, p, dof, expected = chi2_contingency(table)\n",
        "    print('=================================================================================')\n",
        "    print('Data dimension: ', table.shape)\n",
        "    print(f'Chi-square value = {c:.4f}')\n",
        "    print(f'Chi-square p-value = {p:.4f}')\n",
        "    print('=================================================================================\\n')\n",
        "    twosub = data[[var_a, var_b]]\n",
        "    var_b_list = data[var_b].unique().tolist()\n",
        "    all_col_list = []\n",
        "    for i in var_b_list:\n",
        "        Bi = twosub[data[var_b] == i]\n",
        "        Ci = Bi.groupby(var_a).count()\n",
        "        dict = {Ci.columns[0]: f'{Ci.columns[0]}_{str(i)}'}\n",
        "        Ci = Ci.rename(columns=dict)\n",
        "        all_col_list.append(Ci)\n",
        "    d = pd.concat(all_col_list, axis=1, join='outer').fillna(0)\n",
        "    sum_row = d.aggregate('sum', axis=1)\n",
        "    e = pd.concat([d, sum_row], axis=1, join='inner')\n",
        "    e = e.rename(columns={0: 'horizonsum'})\n",
        "    sum_col = e.aggregate('sum', axis=0)\n",
        "    e = pd.concat([e, pd.DataFrame([sum_col], columns=e.columns)], ignore_index=True)\n",
        "    lst_a = data[var_a].unique()\n",
        "    lst_a.sort()\n",
        "    lst_a = lst_a.tolist()\n",
        "    lst_a.append('vertisum')\n",
        "    e[var_a] = lst_a\n",
        "    cols = e.columns.tolist()\n",
        "    cols = cols[-1:] + cols[:-1]\n",
        "    e = e[cols]\n",
        "    cols = list(e.columns)\n",
        "    h = cols[1:-1]\n",
        "    h.sort()\n",
        "    cols[1:-1] = h\n",
        "    e = e[cols]\n",
        "    i = e.iloc[:, 1:].astype(int)\n",
        "    e = e.iloc[:, :1]\n",
        "    e = pd.concat([e, i], axis=1, join='inner')\n",
        "    for i in cols:\n",
        "        if i == 'horizonsum':\n",
        "            continue\n",
        "        if i == var_a:\n",
        "            continue\n",
        "        else:\n",
        "            e['% ' + i] = ((e[i] / e['horizonsum'] * 100)).round(4).astype(float)\n",
        "    print(e.to_string(index=False))\n",
        "    if data[var_b].nunique() == 2:\n",
        "        f = e.columns.tolist()\n",
        "        var = e.iloc[0:-1][f[0]]\n",
        "        outc = e.iloc[0:-1][f[-1]]\n",
        "        fig = plt.figure(figsize=(3, 3))\n",
        "        plt.bar(var, outc, label=var.name)\n",
        "        plt.xlabel(var.name)\n",
        "        plt.xticks(range(int(var.min()), int(var.max()) + 1))\n",
        "        plt.ylabel(outc.name)\n",
        "        plt.title(f'Chi-square p-value: {p: .5f}')\n",
        "        plt.show()\n",
        "\n",
        "def soap_tableone_cat(data, col, target):\n",
        "  all = n_percent(data, col)\n",
        "  val = np.sort(data[col].unique()).tolist()\n",
        "  grouped = data.groupby(target)\n",
        "  grouplist = []\n",
        "  for group_df in grouped:\n",
        "    grouplist.append(group_df)\n",
        "  gr_0 = (grouplist[0])[1] #splited df with target = 0\n",
        "  gr_1 = (grouplist[1])[1] #splited with target = 1\n",
        "\n",
        "  if gr_0[col].nunique() == 2:\n",
        "    no = n_percent(gr_0, col)\n",
        "  elif gr_0[col].nunique() == 1:\n",
        "    if gr_0[col].unique() == 0:\n",
        "      no = n_percent(gr_0, col) + ['0 (0.00%)']\n",
        "    elif gr_0[col].unique() == 1:\n",
        "      no = ['0 (0.00%)'] + n_percent(gr_0, col)\n",
        "\n",
        "  if gr_1[col].nunique() == 2:\n",
        "    yes = n_percent(gr_1, col)\n",
        "  elif gr_1[col].nunique() == 1:\n",
        "    if gr_1[col].unique() == 0:\n",
        "      yes = n_percent(gr_1, col) + ['0 (0.00%)']\n",
        "    elif gr_1[col].unique() == 1:\n",
        "      yes = ['0 (0.00%)'] + n_percent(gr_1, col)\n",
        "\n",
        "\n",
        "\n",
        "  chi_p = chi_pv(data, target, col)\n",
        "  chi = ['%.3f' %chi_p,'']\n",
        "\n",
        "  if chi_p < 0.001:\n",
        "    data = {'All':all, 'Val': val, target+'_neg': no, target+'_pos': yes, 'p-value': ['< 0.001','']}\n",
        "  if chi_p >= 0.001:\n",
        "    data = {'All':all, 'Val': val, target+'_neg': no, target+'_pos': yes, 'p-value': chi}\n",
        "  table_0 = pd.DataFrame(data)\n",
        "  table_0.index.name = ' '\n",
        "  return table_0\n",
        "\n",
        "#Make table one for binary data and a binary target\n",
        "def soap_joined_tableone_cat(data, bicol_list, target):\n",
        "  All = [f\"{len(data)} (100.00%)\"]\n",
        "  Val = ['-']\n",
        "  Name = ['All']\n",
        "  grouped = data.groupby(target)\n",
        "  grouplist = []\n",
        "  for group_df in grouped:\n",
        "    grouplist.append(group_df)\n",
        "  gr_0 = (grouplist[0])[1]\n",
        "  gr_1 = (grouplist[1])[1]\n",
        "\n",
        "  No = [f\"{len(gr_0)} ({len(gr_0)*100/len(data):.2f}%)\"]\n",
        "  Yes = [f\"{len(gr_1)} ({len(gr_1)*100/len(data):.2f}%)\"]\n",
        "  Chi = ['-']\n",
        "  if target in bicol_list:\n",
        "    bicol_list.remove(target)\n",
        "\n",
        "  for col in bicol_list:\n",
        "    all = n_percent(data, col)\n",
        "    val = np.sort(data[col].unique()).tolist()\n",
        "    if gr_0[col].nunique() == 2:\n",
        "      no = n_percent(gr_0, col)\n",
        "    elif gr_0[col].nunique() == 1:\n",
        "      if gr_0[col].unique() == 0:\n",
        "        no = n_percent(gr_0, col) + ['0 (0.00%)']\n",
        "      elif gr_0[col].unique() == 1:\n",
        "        no = ['0 (0.00%)'] + n_percent(gr_0, col) #Corrected here already\n",
        "    if gr_1[col].nunique() == 2:\n",
        "      yes = n_percent(gr_1, col)\n",
        "    elif gr_1[col].nunique() == 1:\n",
        "      if gr_1[col].unique() == 0:\n",
        "        yes = n_percent(gr_1, col) + ['0 (0.00%)']\n",
        "      elif gr_1[col].unique() == 1:\n",
        "        yes = ['0 (0.00%)'] + n_percent(gr_1, col)\n",
        "    name = [f'{col}', '']\n",
        "    chi_p = chi_pv(data, target, col)\n",
        "    if chi_p < 0.001:\n",
        "      chi = ['<0.001','']\n",
        "    elif chi_p >= 0.001:\n",
        "      chi = ['%.3f' %chi_p,'']\n",
        "\n",
        "    All.extend(all)\n",
        "    No.extend(no)\n",
        "    Yes.extend(yes)\n",
        "    Chi.extend(chi)\n",
        "    Name.extend(name)\n",
        "    Val.extend(val)\n",
        "  data = {'Parameter':Name, 'Val': Val, 'All':All, f'{target}_neg': No, f'{target}_pos': Yes, 'p-value': Chi}\n",
        "  table_1 = pd.DataFrame(data)\n",
        "  table_1.index.name = ' '\n",
        "  return table_1\n",
        "\n",
        "#TTest and Mann-Whitney-U test\n",
        "def soap_TU(data, col, target):\n",
        "\tif data[target].nunique() != 2:\n",
        "\t\tprint(f'The outcome {target} is non-binary')\n",
        "\telse:\n",
        "\t\tvar_by_outcome = data.groupby(target)[col].describe()\n",
        "\t\tprint(col,'\\n', var_by_outcome)\n",
        "\t\tcat1 = data[data[target] == 0]\n",
        "\t\tcat2 = data[data[target] == 1]\n",
        "\t\tprint('-----------------------------------------------------------------\\n')\n",
        "\t\tprint(stats.ttest_ind(cat1[col].dropna(), cat2[col].dropna()))\n",
        "\t\tprint(stats.mannwhitneyu(cat1[col].dropna(), cat2[col].dropna()))\n",
        "\n",
        "\t\tplt.boxplot([cat1[col].dropna(), cat2[col].dropna()], showmeans = True)\n",
        "\t\tplt.show()\n",
        "\n",
        "def soap_meansd(data, col):\n",
        "  ms = f'{data[col].mean():.2f}({data[col].std():.2f})'\n",
        "  return ms\n",
        "\n",
        "def soap_tableone_meansd(data, col, target):\n",
        "  all = soap_meansd(data, col)\n",
        "  grouped = data.groupby(target)\n",
        "  grouplist = []\n",
        "  for group_df in grouped:\n",
        "    grouplist.append(group_df)\n",
        "  gr_0 = (grouplist[0])[1] #Group with target = 0\n",
        "  gr_1 = (grouplist[1])[1] #Group with target = 1\n",
        "  no = soap_meansd(gr_0, col)\n",
        "  yes = soap_meansd(gr_1, col)\n",
        "\n",
        "  ttest_result = stats.ttest_ind(gr_0[col].dropna(), gr_1[col].dropna()) #ttest of col grouped by target binary\n",
        "  t = ttest_result.pvalue\n",
        "\n",
        "  if t < 0.001:\n",
        "    data = {'All':all, target+'_neg': no, target+'_pos': yes, 'p-value': '< 0.001'}\n",
        "  if t >= 0.001:\n",
        "    data = {'All':all, target+'_neg': no, target+'_pos': yes, 'p-value': f'{t:.3f}'}\n",
        "\n",
        "  table_0 = pd.DataFrame([data], index = [0])\n",
        "  return table_0\n",
        "\n",
        "def soap_numcol_list(data):\n",
        "    num_cols = []\n",
        "    for col in data.columns:\n",
        "        if (data[col].dtype != 'object') & (data[col].nunique() > 5):\n",
        "            num_cols.append(col)\n",
        "    serial_words = ['serial', 'Serial', 'hn', 'HN', 'id', 'ID']\n",
        "    num_cols = [item for item in num_cols if item not in serial_words]\n",
        "    return num_cols\n",
        "\n",
        "def soap_joined_tableone_meansd(data, numcol, target):\n",
        "  Name = []\n",
        "  All = []\n",
        "  No = []\n",
        "  Yes = []\n",
        "  P = []\n",
        "  for col in numcol:\n",
        "    Name.append(col)\n",
        "    All.append(soap_meansd(data, col))\n",
        "    grouped = data.groupby(target)\n",
        "    grouplist = []\n",
        "    for group_df in grouped:\n",
        "      grouplist.append(group_df)\n",
        "    gr_0 = (grouplist[0])[1] #Group with target = 0\n",
        "    gr_1 = (grouplist[1])[1] #Group with target = 1\n",
        "    No.append(soap_meansd(gr_0, col))\n",
        "    Yes.append(soap_meansd(gr_1, col))\n",
        "    ttest_result = stats.ttest_ind(gr_0[col].dropna(), gr_1[col].dropna())\n",
        "    t = ttest_result.pvalue\n",
        "    if t < 0.001:\n",
        "      P.append(f'<0.001')\n",
        "    if t >= 0.001:\n",
        "      P.append(f'{t:.3f}')\n",
        "  data = {'Parameter':Name, 'All':All, f'{target}_neg': No, f'{target}_pos': Yes, 'p-value': P}\n",
        "  table_1 = pd.DataFrame(data)\n",
        "  table_1.index.name = ' '\n",
        "  return table_1\n",
        "\n",
        "def soap_medianiqr(data, col):\n",
        "  mi = f'{data[col].median():.2f}({data[col].quantile(0.25):.2f}-{data[col].quantile(0.75):.2f})'\n",
        "  return mi\n",
        "\n",
        "def soap_tableone_medianiqr(data, col, target):\n",
        "  all = soap_medianiqr(data, col)\n",
        "  grouped = data.groupby(target)\n",
        "  grouplist = []\n",
        "  for group_df in grouped:\n",
        "    grouplist.append(group_df)\n",
        "  gr_0 = (grouplist[0])[1] #Group with target = 0\n",
        "  gr_1 = (grouplist[1])[1] #Group with target = 1\n",
        "  no = soap_medianiqr(gr_0, col)\n",
        "  yes = soap_medianiqr(gr_1, col)\n",
        "\n",
        "  utest_result = stats.mannwhitneyu(gr_0[col].dropna(), gr_1[col].dropna()) #Mann-Whitney-U test of col grouped by target binary\n",
        "  u = utest_result.pvalue\n",
        "\n",
        "  if u < 0.001:\n",
        "    data = {'All':all, target+'_neg': no, target+'_pos': yes, 'p-value': '< 0.001'}\n",
        "  if u >= 0.001:\n",
        "    data = {'All':all, target+'_neg': no, target+'_pos': yes, 'p-value': f'{u:.3f}'}\n",
        "\n",
        "  table_0 = pd.DataFrame([data], index = [0])\n",
        "  return table_0\n",
        "\n",
        "def soap_joined_tableone_medianiqr(data, numcol, target):\n",
        "  Name = []\n",
        "  All = []\n",
        "  No = []\n",
        "  Yes = []\n",
        "  P = []\n",
        "  for col in numcol:\n",
        "    Name.append(col)\n",
        "    All.append(soap_medianiqr(data, col))\n",
        "    grouped = data.groupby(target)\n",
        "    grouplist = []\n",
        "    for group_df in grouped:\n",
        "      grouplist.append(group_df)\n",
        "    gr_0 = (grouplist[0])[1] #Group with target = 0\n",
        "    gr_1 = (grouplist[1])[1] #Group with target = 1\n",
        "    No.append(soap_medianiqr(gr_0, col))\n",
        "    Yes.append(soap_medianiqr(gr_1, col))\n",
        "    utest_result = stats.mannwhitneyu(gr_0[col].dropna(), gr_1[col].dropna()) #Mann-Whitney-U test of col grouped by target binary\n",
        "    u = utest_result.pvalue\n",
        "    if u < 0.001:\n",
        "      P.append(f'<0.001')\n",
        "    if u >= 0.001:\n",
        "      P.append(f'{u:.3f}')\n",
        "  data = {'Parameter':Name, 'All':All, f'{target}_neg': No, f'{target}_pos': Yes, 'p-value': P}\n",
        "  table_1 = pd.DataFrame(data)\n",
        "  table_1.index.name = ' '\n",
        "  return table_1\n",
        "\n",
        "def soap_autotableone(data, target):\n",
        "\n",
        "  bicol_list = soap_bicol_list(data)\n",
        "\n",
        "  All = [f\"{len(data)} (100.00%)\"]\n",
        "  Val = ['-']\n",
        "  Name = ['All']\n",
        "\n",
        "  grouped = data.groupby(target)\n",
        "  grouplist = []\n",
        "  for group_df in grouped:\n",
        "    grouplist.append(group_df)\n",
        "  gr_0 = (grouplist[0])[1]\n",
        "  gr_1 = (grouplist[1])[1]\n",
        "\n",
        "  No = [f\"{len(gr_0)} ({len(gr_0)*100/len(data):.2f}%)\"]\n",
        "  Yes = [f\"{len(gr_1)} ({len(gr_1)*100/len(data):.2f}%)\"]\n",
        "  P = ['-']\n",
        "\n",
        "  if target in bicol_list:\n",
        "    bicol_list.remove(target)\n",
        "  for col in bicol_list:\n",
        "    all = n_percent(data, col)\n",
        "    val = np.sort(data[col].unique()).tolist()\n",
        "    if gr_0[col].nunique() == 2:\n",
        "      no = n_percent(gr_0, col)\n",
        "    elif gr_0[col].nunique() == 1:\n",
        "      if gr_0[col].unique() == 0:\n",
        "        no = n_percent(gr_0, col) + ['0 (0.00%)']\n",
        "      elif gr_0[col].unique() == 1:\n",
        "        no = ['0 (0.00%)'] + n_percent(gr_0, col) #Corrected here already\n",
        "    if gr_1[col].nunique() == 2:\n",
        "      yes = n_percent(gr_1, col)\n",
        "    elif gr_1[col].nunique() == 1:\n",
        "      if gr_1[col].unique() == 0:\n",
        "        yes = n_percent(gr_1, col) + ['0 (0.00%)']\n",
        "      elif gr_1[col].unique() == 1:\n",
        "        yes = ['0 (0.00%)'] + n_percent(gr_1, col)\n",
        "    name = [f'{col}', '']\n",
        "    chi_p = chi_pv(data, target, col)\n",
        "    if chi_p < 0.001:\n",
        "      chi = ['<0.001','']\n",
        "    elif chi_p >= 0.001:\n",
        "      chi = ['%.3f' %chi_p,'']\n",
        "\n",
        "    All.extend(all)\n",
        "    No.extend(no)\n",
        "    Yes.extend(yes)\n",
        "    P.extend(chi)\n",
        "    Name.extend(name)\n",
        "    Val.extend(val)\n",
        "\n",
        "    numcol = soap_numcol_list(data) #Extract numeric column excluding serial words\n",
        "    normnumcol = [] #For normal distribution\n",
        "    notnormnumcol = [] #For not_normal distribution\n",
        "    for col in numcol:\n",
        "      series_float64 = data[col].astype('float64').dropna()\n",
        "      stat, p_value = shapiro(series_float64)\n",
        "      if p_value >= 0.05:\n",
        "        normnumcol.append(col)\n",
        "      else:\n",
        "        notnormnumcol.append(col)\n",
        "\n",
        "  for col in notnormnumcol:\n",
        "    Name.append(col)\n",
        "    All.append(soap_medianiqr(data, col))\n",
        "    Val.append('-')\n",
        "    No.append(soap_medianiqr(gr_0, col))\n",
        "    Yes.append(soap_medianiqr(gr_1, col))\n",
        "    utest_result = stats.mannwhitneyu(gr_0[col].dropna(), gr_1[col].dropna()) #Mann-Whitney-U test of col grouped by target binary\n",
        "    u = utest_result.pvalue\n",
        "    if u < 0.001:\n",
        "      P.append(f'<0.001')\n",
        "    if u >= 0.001:\n",
        "      P.append(f'{u:.3f}')\n",
        "\n",
        "  for col in normnumcol:\n",
        "    Name.append(col)\n",
        "    All.append(soap_meansd(data, col))\n",
        "    Val.append('-')\n",
        "    No.append(soap_meansd(gr_0, col))\n",
        "    Yes.append(soap_meansd(gr_1, col))\n",
        "    ttest_result = stats.ttest_ind(gr_0[col].dropna(), gr_1[col].dropna())\n",
        "    t = ttest_result.pvalue\n",
        "    if t < 0.001:\n",
        "      P.append(f'<0.001')\n",
        "    if t >= 0.001:\n",
        "      P.append(f'{t:.3f}')\n",
        "\n",
        "  data = {'Parameter':Name, 'Val': Val, 'All':All, f'{target}_neg': No, f'{target}_pos': Yes, 'p-value': P}\n",
        "  table_1 = pd.DataFrame(data)\n",
        "  table_1.index.name = ' '\n",
        "  return table_1\n",
        "\n",
        "#Univariate logistic regression\n",
        "\n",
        "def soap_unilogit(df, target, feature):\n",
        "    df['intercept'] = 1.0\n",
        "    X = df[[feature, 'intercept']]  # Independent variable(s)\n",
        "    y = df[target]                  # Dependent variable\n",
        "    model = sm.Logit(y, X).fit()\n",
        "    params = model.params\n",
        "    odds_ratios = np.exp(params)\n",
        "    conf = model.conf_int()\n",
        "    conf['Odds Ratio'] = odds_ratios\n",
        "    conf.columns = ['2.5%', '97.5%', 'Odds Ratio']\n",
        "    conf[['2.5%', '97.5%']] = np.exp(conf[['2.5%', '97.5%']])\n",
        "    conf['p-value'] = model.pvalues\n",
        "    conf = conf.applymap(lambda x: f'{x:.4f}')\n",
        "    stat = conf[['Odds Ratio', '2.5%', '97.5%', 'p-value']]\n",
        "    print(stat)\n",
        "\n",
        "def soap_batch_unilogit(df, target, feature_list):\n",
        "    le = LabelEncoder()\n",
        "    df['intercept'] = 1.0\n",
        "    Name = []\n",
        "    OR = []\n",
        "    CI = []\n",
        "    P = []\n",
        "    for feature in feature_list:\n",
        "        if feature == target:\n",
        "          continue\n",
        "        elif df[feature].dtype == 'O':\n",
        "            if df[feature].nunique() / len(df) < 0.05:\n",
        "              df[feature] = le.fit_transform(df[feature])\n",
        "        else:\n",
        "          pass\n",
        "        X = df[[feature, 'intercept']]  # Independent variable(s)\n",
        "        y = df[target]                  # Dependent variable\n",
        "        model = sm.Logit(y, X).fit(disp=0)\n",
        "        params = model.params\n",
        "        odds_ratios = np.exp(params)\n",
        "        conf = model.conf_int()\n",
        "        conf['Odds Ratio'] = odds_ratios\n",
        "        conf.columns = ['2.5%', '97.5%', 'Odds Ratio']\n",
        "        conf[['2.5%', '97.5%']] = np.exp(conf[['2.5%', '97.5%']])\n",
        "        conf['p-value'] = model.pvalues\n",
        "\n",
        "        # Keep the formatting to where it's needed and avoid global applymap\n",
        "        OR_val = f'{odds_ratios[feature]:.4f}'\n",
        "        CI_val = f\"({conf.loc[feature, '2.5%']:.4f}-{conf.loc[feature, '97.5%']:.4f})\"\n",
        "        P_val = conf.loc[feature, 'p-value']\n",
        "\n",
        "        if P_val < 0.001:\n",
        "            P_val_formatted = \"<0.001\"\n",
        "        else:\n",
        "            P_val_formatted = f\"{P_val:.4f}\"\n",
        "\n",
        "        Name.append(feature)\n",
        "        OR.append(OR_val)\n",
        "        CI.append(CI_val)\n",
        "        P.append(P_val_formatted)\n",
        "\n",
        "    data = {'Parameter': Name, 'Crude OR': OR, '95% CI': CI, 'p-value': P}\n",
        "    table_1 = pd.DataFrame(data)\n",
        "    table_1.index.name = ' '\n",
        "    print(f'Univariate logistic regression analysis')\n",
        "    print(table_1)\n",
        "\n",
        "def soap_batch_unilogit_graph(df, target, feature_list):\n",
        "    df['intercept'] = 1.0\n",
        "    le = LabelEncoder()\n",
        "    Lower = []\n",
        "    Upper = []\n",
        "    OR = []\n",
        "    Name = []\n",
        "    for feature in feature_list:\n",
        "        if feature == target:\n",
        "          continue\n",
        "        elif df[feature].dtype == 'O':\n",
        "            if df[feature].nunique() / len(df) < 0.05:\n",
        "              df[feature] = le.fit_transform(df[feature])\n",
        "        else:\n",
        "          pass\n",
        "        X = df[[feature, 'intercept']]  # Independent variable(s)\n",
        "        y = df[target]                  # Dependent variable\n",
        "        model = sm.Logit(y, X).fit(disp=0)\n",
        "        params = model.params\n",
        "        odds_ratios = np.exp(params)\n",
        "        conf = model.conf_int()\n",
        "        conf['Odds Ratio'] = odds_ratios\n",
        "        conf.columns = ['2.5%', '97.5%', 'Odds Ratio']\n",
        "        conf[['2.5%', '97.5%']] = np.exp(conf[['2.5%', '97.5%']])\n",
        "        ood = odds_ratios[feature]\n",
        "        low = conf.loc[feature, '2.5%']\n",
        "        up = conf.loc[feature, '97.5%']\n",
        "\n",
        "        Name.append(feature)\n",
        "        Lower.append(low)\n",
        "        Upper.append(up)\n",
        "        OR.append(ood)\n",
        "    data = {'var': Name, 'OR': OR, '2.5%': Lower, '97.5%': Upper}\n",
        "    val = pd.DataFrame(data)\n",
        "    val.set_index('var')\n",
        "    plt.figure(figsize=(5, 10))  # Adjust the figure size as needed\n",
        "\n",
        "    # Directly use 'var' DataFrame, ensure it's correctly defined and not None\n",
        "    plt.errorbar(val['OR'], val['var'], xerr=[val['OR'] - val['2.5%'], val['97.5%'] - val['OR']], fmt='o', color='b', capsize=5, linestyle='None', marker='o')\n",
        "    plt.axvline(x=1, linestyle='--', color='r', linewidth=1)\n",
        "    plt.yticks(rotation=0)  # Adjust rotation if needed\n",
        "    plt.xlabel('Odds Ratio (95% CI)')\n",
        "    plt.ylabel('Variables')\n",
        "    plt.xticks(rotation=90)  # Adjust for better label readability\n",
        "    plt.title('Odds Ratios and 95% Confidence Intervals from Univariable Logistic Regression')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def soap_roc(data, var, target):\n",
        "  X = np.array(data[var]).reshape(-1, 1)\n",
        "  Y = data[target]\n",
        "  model_lr = LogisticRegression()\n",
        "  model_lr.fit(X,Y)\n",
        "  false_positive_rate, true_positive_rate, threshold = roc_curve(Y, X)\n",
        "  print('roc_auc_score for Logistic Regression: ', roc_auc_score(Y, X))\n",
        "\n",
        "  plt.subplots(1, figsize=(10,10))\n",
        "  plt.title('Receiver Operating Characteristic - Logistic regression')\n",
        "  plt.plot(false_positive_rate, true_positive_rate)\n",
        "  plt.plot([0, 1], ls=\"--\")\n",
        "  plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
        "  plt.ylabel('Sensitivity')\n",
        "  plt.xlabel('1-Specificity')\n",
        "  plt.show()\n",
        "\n",
        "  roctab = roc_curve(Y, X)\n",
        "  roctab\n",
        "  roctable = pd.DataFrame({'Cutpoint': roctab[2], 'Specificity': 1-roctab[0], 'Sensitivity': roctab[1]})\n",
        "  print(roctable)\n",
        "\n",
        "def soap_multiple_lr(data, target):\n",
        "  X = pd.get_dummies(data.drop(target, axis=1), drop_first=True)\n",
        "  X.fillna(X.mean(), inplace=True)\n",
        "  y = data[target]\n",
        "  X = sm.add_constant(X)\n",
        "  model = sm.Logit(y, X).fit(disp=0)\n",
        "  print(model.summary())\n",
        "  params = model.params\n",
        "  conf = model.conf_int()\n",
        "  conf['adjusted OR'] = params\n",
        "  conf.columns = ['2.5%','97.5%', 'adjusted OR']\n",
        "  np.set_printoptions(precision=4, suppress=True)\n",
        "  val = np.exp(conf)\n",
        "  val = val.applymap(lambda x: f'{x:.4f}')\n",
        "  val['p-value'] = model.pvalues\n",
        "  P = []\n",
        "  for p in val['p-value']:\n",
        "    if p < 0.001:\n",
        "      formatted_p = \"<0.001\"\n",
        "    else:\n",
        "      formatted_p = f\"{p:.4f}\"\n",
        "    P.append(formatted_p)\n",
        "  val['p-value'] = P\n",
        "  stat = val[['adjusted OR', '2.5%', '97.5%', 'p-value']]\n",
        "  print(stat)\n",
        "\n",
        "def soap_multiple_lr_hgraph(data, target):\n",
        "  X = pd.get_dummies(data.drop(target, axis=1), drop_first=True)\n",
        "  X.fillna(X.mean(), inplace=True)\n",
        "  y = data[target]\n",
        "  X = sm.add_constant(X)\n",
        "  model = sm.Logit(y, X).fit(disp=0)\n",
        "  params = model.params\n",
        "  conf = model.conf_int()\n",
        "  conf['adjusted OR'] = params\n",
        "  conf.columns = ['2.5%','97.5%', 'adjusted OR']\n",
        "  np.set_printoptions(precision=4, suppress=True)\n",
        "  val = np.exp(conf)\n",
        "\n",
        "  plt.figure(figsize=(15, 5))\n",
        "  val = val[1:]\n",
        "  plt.errorbar(val.index, val['adjusted OR'], yerr=[val['adjusted OR'] - val['2.5%'], val['97.5%'] - val['adjusted OR']], fmt='o', color='b', capsize=5)\n",
        "  plt.axhline(y=1, linestyle='--', color='r', linewidth=1)\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.xlabel('Variables')\n",
        "  plt.ylabel('adjusted Odds Ratio (95% CI)')\n",
        "  plt.yticks(rotation=90)\n",
        "  plt.title('Adjusted Odds Ratios and 95% Confidence Intervals from Multivariable Logistic Regression')\n",
        "  plt.tight_layout()\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def soap_multiple_lr_vgraph(data, target):\n",
        "    X = pd.get_dummies(data.drop(target, axis=1), drop_first=True)\n",
        "    X.fillna(X.mean(), inplace=True)\n",
        "    y = data[target]\n",
        "    X = sm.add_constant(X)\n",
        "    model = sm.Logit(y, X).fit(disp=0)\n",
        "    params = model.params\n",
        "    conf = model.conf_int()\n",
        "    conf['adjusted OR'] = params\n",
        "    conf.columns = ['2.5%', '97.5%', 'adjusted OR']\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    val = np.exp(conf)\n",
        "\n",
        "    plt.figure(figsize=(5, 15))  # Adjust the figure size as needed\n",
        "    val = val[1:]  # Exclude the intercept for plotting\n",
        "    # Use 'plt.errorbar' with vertical orientation\n",
        "    plt.errorbar(val['adjusted OR'], val.index, xerr=[val['adjusted OR'] - val['2.5%'], val['97.5%'] - val['adjusted OR']], fmt='o', color='b', capsize=5, linestyle='None', marker='o')\n",
        "    plt.axvline(x=1, linestyle='--', color='r', linewidth=1)\n",
        "    plt.yticks(rotation=0)  # Adjust rotation if needed\n",
        "    plt.xlabel('Adjusted Odds Ratio (95% CI)')\n",
        "    plt.ylabel('Variables')\n",
        "    plt.xticks(rotation=90)  # Adjust for better label readability\n",
        "    plt.title('Adjusted Odds Ratios and 95% Confidence Intervals from Multivariable Logistic Regression')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "#Draw a Kaplan-Meier curve\n",
        "def soap_single_kmc_(data, status, interval, max_time=None):\n",
        "    sta = data[status].to_list()\n",
        "    sta_bool = [bool(item) for item in sta]\n",
        "    interv = data[interval]\n",
        "\n",
        "\n",
        "    # If max_time is not provided, use the maximum value of interv\n",
        "    if max_time is None:\n",
        "        max_time = interv.max()\n",
        "\n",
        "    kmf = KaplanMeierFitter()\n",
        "    kmf.fit(durations=interv, event_observed=sta_bool)  # Fit the data into the model\n",
        "    time, survival_prob = kmf.survival_function_.index, kmf.survival_function_[\"KM_estimate\"]\n",
        "\n",
        "    kmf.plot()\n",
        "    plt.title('Kaplan-Meier Survival Curves')\n",
        "    plt.ylabel(\"est. probability of survival\")\n",
        "    plt.xlabel(\"time \")\n",
        "\n",
        "    plt.xlim(0, max_time)  # Use max_time to set the x-axis limit\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "#Get survival ladder\n",
        "def soap_survival_ladder(data, status, interval): #Get time-point specific survival probability with the closest 95%CI\n",
        "  kmf = KaplanMeierFitter()\n",
        "  interval = data[interval]\n",
        "  event = data[status]\n",
        "  kmf.fit(interval, event_observed=event)\n",
        "  time_points = [30, 365, 730, 1095, 1460, 1825]\n",
        "  survival_prob = kmf.predict(time_points)\n",
        "  kmf_times = kmf.survival_function_.index.to_numpy()\n",
        "\n",
        "  closest_time_points = np.searchsorted(kmf_times, time_points, side='left')\n",
        "  closest_time_points = np.minimum(closest_time_points, len(kmf_times) - 1)\n",
        "  closest_kmf_times = kmf_times[closest_time_points]\n",
        "  closest_confidence_intervals = kmf.confidence_interval_.loc[closest_kmf_times]\n",
        "  closest_confidence_intervals.reset_index(inplace=True)\n",
        "  closest_confidence_intervals.rename(columns={'index': 'Closest_days'}, inplace=True)\n",
        "\n",
        "  survival_prob_df = survival_prob.to_frame(name='Survival Probability')\n",
        "  survival_prob_df.reset_index(inplace=True)\n",
        "  survival_prob_df.rename(columns={'index': 'Days'}, inplace=True)\n",
        "  survival_prob_df['95%CI lower'] = closest_confidence_intervals['KM_estimate_lower_0.95']\n",
        "  survival_prob_df['95%CI upper'] = closest_confidence_intervals['KM_estimate_upper_0.95']\n",
        "  print(survival_prob_df)\n",
        "\n",
        "def soap_multigroup_kp(data, event, interval, intervention):\n",
        "    kmf = KaplanMeierFitter()\n",
        "    groups = data[intervention].unique()\n",
        "    for group in groups:\n",
        "        data_gr = data[data[intervention] == group]\n",
        "        kmf.fit(data_gr[interval], event_observed=data_gr[event], label=group)\n",
        "        kmf.plot()\n",
        "    plt.title('Kaplan-Meier Survival Curves')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Survival Probability')\n",
        "    plt.legend(title=intervention)\n",
        "    plt.show()\n",
        "\n",
        "def soap_logrank(data, event, interval, intervention):\n",
        "    if data[intervention].nunique() != 2:\n",
        "        print('As the factor', intervention, 'is non-binary, the Logrank statistics is not calculated.')\n",
        "    else:\n",
        "        interval_0 = []\n",
        "        group_0 = data[data[intervention] == 0]\n",
        "        for i in group_0[interval]:\n",
        "            interval_0.append(i)\n",
        "        T = interval_0\n",
        "\n",
        "        interval_1 = []\n",
        "        group_1 = data[data[intervention] == 1]\n",
        "        for i in group_1[interval]:\n",
        "            interval_1.append(i)\n",
        "        T1 = interval_1\n",
        "\n",
        "        censor_0 = []\n",
        "        for j in group_0[event]:\n",
        "            censor_0.append(j)\n",
        "        E = censor_0\n",
        "\n",
        "        censor_1 = []\n",
        "        for k in group_1[event]:\n",
        "            censor_1.append(k)\n",
        "        E1 = censor_1\n",
        "        results = logrank_test(T, T1, E, E1)\n",
        "        if results.p_value >= 0.05:\n",
        "            print('Log-rank for', intervention,' gives p-value at: %.5f' % results.p_value)\n",
        "        else:\n",
        "            print('Log-rank for', intervention,' gives p-value at: %.5f' % results.p_value, '**')\n",
        "\n",
        "#Batch log-rank test\n",
        "def soap_batchlogrank(data, event, interval, batch):\n",
        "    for i in batch:\n",
        "        soap_logrank(data, event, interval, i)\n",
        "\n",
        "#Cox's proportional hazard analysis\n",
        "\n",
        "def soap_single_chr(data, event, interval, intervention):\n",
        "    from lifelines import CoxPHFitter\n",
        "    markers = []\n",
        "    markers.append(intervention)\n",
        "    markers.extend([interval, event])\n",
        "    data = data[markers] #Feature selection\n",
        "\n",
        "    cph = CoxPHFitter()\n",
        "    cph.fit(data, duration_col= interval, event_col=event)\n",
        "    summary = cph.summary\n",
        "    summary_table = summary[['exp(coef)','exp(coef) lower 95%', 'exp(coef) upper 95%']]\n",
        "    summary_table = summary_table.rename(columns={'exp(coef)': 'Hazard Ratio', 'exp(coef) lower 95%': '95%CI lower', 'exp(coef) upper 95%': '95%CI upper'}, errors=\"raise\")\n",
        "\n",
        "    print(summary_table)\n",
        "\n",
        "#Batch Cox's regression\n",
        "def soap_batchcox(data, event, interval, batch):\n",
        "    for i in batch:\n",
        "        soap_single_chr(data, event, interval, i)\n",
        "\n",
        "#Multivariable Cox's proportional hazard analysis\n",
        "def soap_multi_chr(data, event, interval, intervention_list):\n",
        "    from lifelines import CoxPHFitter\n",
        "    markers = intervention_list\n",
        "    markers.extend([interval, event])\n",
        "    data = data[markers] #Feature selection\n",
        "\n",
        "    cph = CoxPHFitter()\n",
        "    cph.fit(data, duration_col= interval, event_col=event)\n",
        "    summary = cph.summary\n",
        "    summary_table = summary[['exp(coef)','exp(coef) lower 95%', 'exp(coef) upper 95%']]\n",
        "    summary_table = summary_table.rename(columns={'exp(coef)': 'Hazard Ratio', 'exp(coef) lower 95%': '95%CI lower', 'exp(coef) upper 95%': '95%CI upper'}, errors=\"raise\")\n",
        "\n",
        "    print(summary_table)"
      ]
    }
  ]
}